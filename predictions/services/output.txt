Epoch 2/16
38/38 - 3s - loss: 1.5710 - amount_loss: 1.2152 - category_loss: 0.2257 - temporal_loss: 0.2108 - category_accuracy: 0.2017 - val_loss: 1.0717 - val_amount_loss: 0.6753 - val_category_loss: 0.1965 - val_temporal_loss: 0.1398 - val_category_accuracy: 0.2465 - lr: 1.0000e-04 - 3s/epoch - 88ms/step
Epoch 3/16
38/38 - 3s - loss: 1.5066 - amount_loss: 1.1594 - category_loss: 0.2175 - temporal_loss: 0.1927 - category_accuracy: 0.2206 - val_loss: 1.0400 - val_amount_loss: 0.6469 - val_category_loss: 0.1936 - val_temporal_loss: 0.1295 - val_category_accuracy: 0.2657 - lr: 1.0000e-04 - 3s/epoch - 88ms/step
Epoch 4/16
38/38 - 4s - loss: 1.4799 - amount_loss: 1.1447 - category_loss: 0.2119 - temporal_loss: 0.1764 - category_accuracy: 0.2413 - val_loss: 1.0259 - val_amount_loss: 0.6383 - val_category_loss: 0.1909 - val_temporal_loss: 0.1250 - val_category_accuracy: 0.2692 - lr: 1.0000e-04 - 4s/epoch - 93ms/step
Epoch 5/16
38/38 - 3s - loss: 1.4607 - amount_loss: 1.1366 - category_loss: 0.2079 - temporal_loss: 0.1569 - category_accuracy: 0.2442 - val_loss: 1.0136 - val_amount_loss: 0.6273 - val_category_loss: 0.1903 - val_temporal_loss: 0.1207 - val_category_accuracy: 0.3077 - lr: 1.0000e-04 - 3s/epoch - 89ms/step
Epoch 6/16
38/38 - 3s - loss: 1.4490 - amount_loss: 1.1332 - category_loss: 0.2067 - temporal_loss: 0.1302 - category_accuracy: 0.2577 - val_loss: 1.0108 - val_amount_loss: 0.6255 - val_category_loss: 0.1900 - val_temporal_loss: 0.1247 - val_category_accuracy: 0.3112 - lr: 1.0000e-04 - 3s/epoch - 88ms/step
Epoch 7/16
38/38 - 3s - loss: 1.4382 - amount_loss: 1.1289 - category_loss: 0.2046 - temporal_loss: 0.1196 - category_accuracy: 0.2611 - val_loss: 1.0031 - val_amount_loss: 0.6230 - val_category_loss: 0.1887 - val_temporal_loss: 0.1146 - val_category_accuracy: 0.3077 - lr: 1.0000e-04 - 3s/epoch - 90ms/step
Epoch 8/16
38/38 - 3s - loss: 1.4209 - amount_loss: 1.1198 - category_loss: 0.2007 - temporal_loss: 0.1121 - category_accuracy: 0.2859 - val_loss: 0.9966 - val_amount_loss: 0.6221 - val_category_loss: 0.1866 - val_temporal_loss: 0.1120 - val_category_accuracy: 0.3094 - lr: 1.0000e-04 - 3s/epoch - 89ms/step
Epoch 9/16
38/38 - 3s - loss: 1.4111 - amount_loss: 1.1259 - category_loss: 0.1948 - temporal_loss: 0.0989 - category_accuracy: 0.2872 - val_loss: 0.9914 - val_amount_loss: 0.6160 - val_category_loss: 0.1876 - val_temporal_loss: 0.1085 - val_category_accuracy: 0.3094 - lr: 1.0000e-04 - 3s/epoch - 90ms/step
Epoch 10/16
38/38 - 3s - loss: 1.4084 - amount_loss: 1.1194 - category_loss: 0.1967 - temporal_loss: 0.1019 - category_accuracy: 0.2897 - val_loss: 0.9850 - val_amount_loss: 0.6160 - val_category_loss: 0.1856 - val_temporal_loss: 0.1020 - val_category_accuracy: 0.3024 - lr: 1.0000e-04 - 3s/epoch - 90ms/step
Epoch 11/16
38/38 - 3s - loss: 1.4005 - amount_loss: 1.1179 - category_loss: 0.1943 - temporal_loss: 0.0970 - category_accuracy: 0.3019 - val_loss: 0.9810 - val_amount_loss: 0.6152 - val_category_loss: 0.1854 - val_temporal_loss: 0.0944 - val_category_accuracy: 0.3024 - lr: 1.0000e-04 - 3s/epoch - 88ms/step
Epoch 12/16
38/38 - 3s - loss: 1.3965 - amount_loss: 1.1174 - category_loss: 0.1937 - temporal_loss: 0.0923 - category_accuracy: 0.3082 - val_loss: 0.9771 - val_amount_loss: 0.6142 - val_category_loss: 0.1848 - val_temporal_loss: 0.0921 - val_category_accuracy: 0.3042 - lr: 1.0000e-04 - 3s/epoch - 88ms/step
Epoch 13/16
38/38 - 3s - loss: 1.3875 - amount_loss: 1.1143 - category_loss: 0.1918 - temporal_loss: 0.0837 - category_accuracy: 0.3099 - val_loss: 0.9756 - val_amount_loss: 0.6135 - val_category_loss: 0.1863 - val_temporal_loss: 0.0806 - val_category_accuracy: 0.3042 - lr: 1.0000e-04 - 3s/epoch - 88ms/step
Epoch 14/16
38/38 - 3s - loss: 1.3834 - amount_loss: 1.1149 - category_loss: 0.1906 - temporal_loss: 0.0792 - category_accuracy: 0.3225 - val_loss: 0.9670 - val_amount_loss: 0.6132 - val_category_loss: 0.1836 - val_temporal_loss: 0.0704 - val_category_accuracy: 0.3059 - lr: 1.0000e-04 - 3s/epoch - 89ms/step
Epoch 15/16
38/38 - 3s - loss: 1.3783 - amount_loss: 1.1110 - category_loss: 0.1903 - temporal_loss: 0.0784 - category_accuracy: 0.3192 - val_loss: 0.9656 - val_amount_loss: 0.6127 - val_category_loss: 0.1847 - val_temporal_loss: 0.0625 - val_category_accuracy: 0.3007 - lr: 1.0000e-04 - 3s/epoch - 89ms/step
Epoch 16/16
38/38 - 3s - loss: 1.3770 - amount_loss: 1.1126 - category_loss: 0.1895 - temporal_loss: 0.0794 - category_accuracy: 0.3263 - val_loss: 0.9622 - val_amount_loss: 0.6141 - val_category_loss: 0.1835 - val_temporal_loss: 0.0570 - val_category_accuracy: 0.3042 - lr: 1.0000e-04 - 3s/epoch - 92ms/step
W0000 00:00:1745217680.656889     479 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: "Softmax" attr { key: "T" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: "CPU" vendor: "AuthenticAMD" model: "248" frequency: 3393 num_cores: 12 environment { key: "cpu_instruction_set" value: "AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2" } environment { key: "eigen" value: "3.4.90" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 16777216 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }
W0000 00:00:1745217680.656957     479 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: "Softmax" attr { key: "T" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: "CPU" vendor: "AuthenticAMD" model: "248" frequency: 3393 num_cores: 12 environment { key: "cpu_instruction_set" value: "AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2" } environment { key: "eigen" value: "3.4.90" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 16777216 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }
Best Thresholds per class: [0.3  0.4  0.2  0.35 0.25 0.3  0.35 0.25 0.3 ], Best F1 Scores per class: [0.27027027 0.48219178 0.08421053 0.42778793 0.069869   0.06349206
 0.3125     0.10889292 0.41245136]
                precision    recall  f1-score   support

Business lunch       0.19      0.45      0.27        33
        Coffee       0.36      0.75      0.48       118
      Learning       0.04      0.87      0.08        23
        Market       0.28      0.94      0.43       124
         Other       0.04      0.47      0.07        17
         Phone       0.04      0.12      0.06        16
    Restaurant       0.20      0.68      0.31        73
          Taxi       0.06      1.00      0.11        30
     Transport       0.28      0.77      0.41       138

     micro avg       0.17      0.76      0.27       572
     macro avg       0.17      0.67      0.25       572
  weighted avg       0.25      0.76      0.36       572
   samples avg       0.17      0.76      0.28       572

Day 1: {'Coffee': 17.430744, 'Learning': 17.667486, 'Market': 16.908615, 'Other': 17.017174, 'Phone': 16.920237, 'Restaurant': 17.101374, 'Taxi': 16.84064, 'Transport': 16.889704}
Day 2: {'Coffee': 17.15455, 'Market': 16.674616, 'Other': 16.766176, 'Restaurant': 16.795626, 'Taxi': 16.661453, 'Transport': 16.720572}
Day 3: {'Market': 16.474314, 'Restaurant': 16.612806, 'Taxi': 16.687979, 'Transport': 16.583973}
Day 4: {'Market': 16.484552, 'Restaurant': 16.592237, 'Taxi': 16.961092, 'Transport': 16.67126}
Day 5: {'Market': 16.411669, 'Restaurant': 16.56607, 'Taxi': 16.9957, 'Transport': 16.655893}
